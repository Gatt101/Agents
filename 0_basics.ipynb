{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f28990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"**What are LLM Hallucinations?**\\n\\nLLM stands for Large Language Model. These models are artificial intelligence (AI) systems that can understand and generate human-like text. They're like super-smart language assistants that can answer questions, summarize articles, and even create stories.\\n\\n**What are Hallucinations?**\\n\\nIn the context of LLMs, hallucinations refer to when the model generates text that is not based on actual facts or information. It's like the model is making things up or seeing things that aren't really there.\\n\\n**Examples of LLM Hallucinations:**\\n\\n1. **Incorrect facts**: An LLM might say that a famous person was born in a certain city when, in fact, they were born in a different city.\\n2. **Made-up information**: An LLM might say that a company has a new product when, in fact, the company has never announced such a product.\\n3. **Overly confident answers**: An LLM might say that a certain statement is true with 100% certainty when, in fact, there's no evidence to support it.\\n\\n**Why do LLMs Hallucinate?**\\n\\nLLMs hallucinate because they're trained on vast amounts of text data, which can include biases, inaccuracies, and inconsistencies. When the model generates text, it might rely on patterns and associations it's learned from the data, rather than actual facts.\\n\\n**Consequences of LLM Hallucinations**\\n\\nLLMs hallucinations can have serious consequences, such as:\\n\\n1. **Spreading misinformation**: LLMs can spread false information, which can be damaging to individuals, communities, or society as a whole.\\n2. **Losing trust**: If people discover that an LLM is generating inaccurate or made-up information, they may lose trust in the model and its outputs.\\n\\n**Mitigating LLM Hallucinations**\\n\\nTo reduce the risk of LLM hallucinations, developers and users can take steps such as:\\n\\n1. **Verifying information**: Double-checking the accuracy of information generated by an LLM.\\n2. **Using multiple sources**: Consulting multiple sources to confirm the accuracy of information.\\n3. **Training LLMs on high-quality data**: Ensuring that LLMs are trained on accurate and reliable data.\\n\\nBy understanding LLM hallucinations and taking steps to mitigate them, we can use these powerful language models more effectively and responsibly.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 489, 'prompt_tokens': 46, 'total_tokens': 535, 'completion_time': 0.707835051, 'prompt_time': 0.00208174, 'queue_time': 0.05375115, 'total_time': 0.709916791}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--5a030beb-34e4-45a7-905b-5216d327c9ff-0' usage_metadata={'input_tokens': 46, 'output_tokens': 489, 'total_tokens': 535}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq( model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,)\n",
    "\n",
    "llm_response = llm.invoke(\"Explain LLM Hallucinations in simple terms.\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574889c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**What are LLM Hallucinations?**\\n\\nLLM stands for Large Language Model. These models are artificial intelligence (AI) systems that can understand and generate human-like text. They're like super-smart language assistants that can answer questions, summarize articles, and even create stories.\\n\\n**What are Hallucinations?**\\n\\nIn the context of LLMs, hallucinations refer to when the model generates text that is not based on actual facts or information. It's like the model is making things up or seeing things that aren't really there.\\n\\n**Examples of LLM Hallucinations:**\\n\\n1. **Incorrect facts**: An LLM might say that a famous person was born in a certain city when, in fact, they were born in a different city.\\n2. **Made-up information**: An LLM might say that a company has a new product when, in fact, the company has never announced such a product.\\n3. **Overly confident answers**: An LLM might say that a certain statement is true with 100% certainty when, in fact, there's no evidence to support it.\\n\\n**Why do LLMs Hallucinate?**\\n\\nLLMs hallucinate because they're trained on vast amounts of text data, which can include biases, inaccuracies, and inconsistencies. When the model generates text, it might rely on patterns and associations it's learned from the data, rather than actual facts.\\n\\n**Consequences of LLM Hallucinations**\\n\\nLLMs hallucinations can have serious consequences, such as:\\n\\n1. **Spreading misinformation**: LLMs can spread false information, which can be damaging to individuals, communities, or society as a whole.\\n2. **Losing trust**: If people discover that an LLM is generating inaccurate or made-up information, they may lose trust in the model and its outputs.\\n\\n**Mitigating LLM Hallucinations**\\n\\nTo reduce the risk of LLM hallucinations, developers and users can take steps such as:\\n\\n1. **Verifying information**: Double-checking the accuracy of information generated by an LLM.\\n2. **Using multiple sources**: Consulting multiple sources to confirm the accuracy of information.\\n3. **Training LLMs on high-quality data**: Ensuring that LLMs are trained on accurate and reliable data.\\n\\nBy understanding LLM hallucinations and taking steps to mitigate them, we can use these powerful language models more effectively and responsibly.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94dacc16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# chaining in LangChain\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m      6\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that translates \u001b[39m\u001b[38;5;132;01m{input_language}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{output_language}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da536ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the LLM response using the StrOutputParser\n",
    "try:\n",
    "    from output_parsers import StrOutputParser\n",
    "except Exception:\n",
    "    # If module import fails in the notebook environment, define a local fallback\n",
    "    class StrOutputParser:\n",
    "        def parse(self, text):\n",
    "            return (text or \"\").strip()\n",
    "\n",
    "parser = StrOutputParser()\n",
    "parsed = parser.parse(getattr(llm_response, 'content', llm_response))\n",
    "print(\"Parsed answer:\")\n",
    "print(parsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
